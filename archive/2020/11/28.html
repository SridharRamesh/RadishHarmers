<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<!-- This page was generated from TweetParser. -->
<link rel="stylesheet" href="../../styles.css">
</head>
<body>
These are all the tweets and replies that I made on 2020-11-28
<hr>
<p id="1332474553709322246"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@darth_erogenous You&#39;re a cartoonist now, on top of everything else? Leave some skills for the rest of us.</p></blockquote>
<br>
Timestamp: 2020-11-28 0:1:4
<br>
Id: 1332474553709322246
<br>

<hr>

</p>
<p id="1332505636538634248"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe I must grant that that town population fit is amazing.</p></blockquote>
<br>
Timestamp: 2020-11-28 2:4:35
<br>
Id: 1332505636538634248
<br>

<hr>

</p>
<p id="1332506917390872576"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe I have discovered https://t.co/wToymgJPng, for more of this sort of thing.</p></blockquote>
<br>
Timestamp: 2020-11-28 2:9:40
<br>
Id: 1332506917390872576
<br>

<hr>

</p>
<p id="1332509592102821889"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe Out of curiosity, if it&#39;s easy to do so, what do these look like if you graph the full histogram of log(X) instead of just the leading digit of X?</p></blockquote>
<br>
Timestamp: 2020-11-28 2:20:18
<br>
Id: 1332509592102821889
<br>

<hr>

</p>
<p id="1332510610941227008"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe It appears that population of US towns is in there already, as is population of UK towns. The fit for US towns is great, the fit for UK towns less so.</p></blockquote>
<br>
Timestamp: 2020-11-28 2:24:21
<br>
Id: 1332510610941227008
<br>

<hr>

</p>
<p id="1332543528619171851"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe It&#39;s conversely odd to me to say that spanning many orders of magnitude explains a Benford distribution, but whatever.</p></blockquote>
<br>
Timestamp: 2020-11-28 4:35:9
<br>
Id: 1332543528619171851
<br>

<hr>

</p>
<p id="1332543937177939968"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@sigfpe You get a Benfordy distribution for any X = YW with Y and W nearly independent and W nearly log-uniform over a large range (or a range nearly a power of 10), so all that&#39;s needed is one such log-uniformish factor. I wonder what that might be for US town population.</p></blockquote>
<br>
Timestamp: 2020-11-28 4:36:46
<br>
Id: 1332543937177939968
<br>

<hr>

</p>
<p id="1332548600870674433"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">It has been over a month since my last good post. Sources say I don&#39;t have it in me anymore. People have stopped liking me (me, not my posts) in droves.</p></blockquote>
<br>
Timestamp: 2020-11-28 4:55:18
<br>
Id: 1332548600870674433
<br>

<hr>

</p>
<p id="1332740537649127426"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe This only indicates that if log (X) happens to be relatively uniformly distributed over a large range, then Benford&#39;s property nearly holds. But why should log (X) happen to be relatively uniformly distributed over a large range?</p></blockquote>
<br>
Timestamp: 2020-11-28 17:37:59
<br>
Id: 1332740537649127426
<br>

<hr>

</p>
<p id="1332741153121300481"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe Well, for some things this happens of course, and for some things it doesn&#39;t. It&#39;s one possible distribution, to be log-uniform over a large interval.</p></blockquote>
<br>
Timestamp: 2020-11-28 17:40:26
<br>
Id: 1332741153121300481
<br>

<hr>

</p>
<p id="1332742525833121793"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe That is, something could easily span many orders of magnitude without being specifically log-uniform over that range. Lots of things do that too.</p></blockquote>
<br>
Timestamp: 2020-11-28 17:45:53
<br>
Id: 1332742525833121793
<br>

<hr>

</p>
<p id="1332775505758744577"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I construct a rectangle of 16 dots by 24 dots in my mind, then count 1, 2, 3, 4, 5, etc, till reaching 384. https://t.co/WNh3YwzGo7</p></blockquote>
<br>
Timestamp: 2020-11-28 19:56:57
<br>
Id: 1332775505758744577
<br>

<hr>

</p>
<p id="1332810988966567937"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">In case it wasn&#39;t clear exactly what we did, we turned (ax + b) * (cx + d) into Ex^2 + Fx + G, where E = a * c, G = b * d, and F =  E + G - (b - a) * (d - c). The x is our base ten, and the a, b, c, and d are the digits of the numbers we start with.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:56
<br>
Id: 1332810988966567937
<br>

<hr>

</p>
<p id="1332810987804766220"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">This is called Karatsuba multiplication, and I&#39;m not joking when I say it was a major breakthrough. Ordinarily, to multiply a pair of two-digit numbers, you have to do four single-digit multiplications. Here, we got away with three single-digit multiplications.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:56
<br>
Id: 1332810987804766220
<br>

<hr>

</p>
<p id="1332810986718425091"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Historically, it was a major breakthrough in mathematics, discovered in the 1960s, to recognize that this could be tackled like so:<br><br>1 * 2 = Two<br>6 * 4 = Twenty-four<br>Two + Twenty-four - (6 - 1) * (4 - 2) = Sixteen<br><br>So the answer is Two hundred + Sixteen tens + Twenty-Four = 384.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:56
<br>
Id: 1332810986718425091
<br>

<hr>

</p>
<p id="1332810993286717441"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Andrey Kolmogorov, a major mathematician, conjectured in 1960 that the N^2 was as good as it gets, but then his student Anatoly Karatsuba discovered this, the first time in history anyone realized how to multiply long numbers more efficiently than the elementary school way.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:57
<br>
Id: 1332810993286717441
<br>

<hr>

</p>
<p id="1332810992376553480"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">(Plus some addition and subtraction steps, but not much more of those in either method, on top of the single-digit multiplication steps. It&#39;s often reasonable to gauge the overall time taken by just looking at the number of single-digit multiplication steps.)</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:57
<br>
Id: 1332810992376553480
<br>

<hr>

</p>
<p id="1332810991139164161"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">When multiplying two N-digit numbers, the familiar elementary school method requires N^2 many single-digit multiplications, while this iterated Karatsuba method gets away with essentially N^(log(3)/log(2))) â‰ˆ N^1.58 many single-digit multiplication steps.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:57
<br>
Id: 1332810991139164161
<br>

<hr>

</p>
<p id="1332810989809643521"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Even for numbers with more than two digits, we can use this same Karatsuba idea to turn multiplying a pair of long numbers into three multiplications of numbers with half as many digits. And then turn each of those into three multiplications of quarter-length numbers, and so on.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:57
<br>
Id: 1332810989809643521
<br>

<hr>

</p>
<p id="1332810997946527744"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">These later ideas are not so much harder than Karatsuba&#39;s idea to explain either, but I know no one cares after the first few posts of a thread like this. They&#39;re also only really a win for very long numbers, because they increase the overhead of the addition etc steps a bit.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:58
<br>
Id: 1332810997946527744
<br>

<hr>

</p>
<p id="1332810997128687623"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Like everyone one-upping each other&#39;s speed records right after Bannister proved the sub-four minute mile possible at all.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:58
<br>
Id: 1332810997128687623
<br>

<hr>

</p>
<p id="1332810995832680450"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">And then people realized how to connect this to the Fast Fourier Transform to make it even more efficient, on the order of N log(N) many single-digit multiplication steps.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:58
<br>
Id: 1332810995832680450
<br>

<hr>

</p>
<p id="1332810994540814336"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">That&#39;s not the end of the story, though. Shortly after that, yet another Russian, Andrei Toom, realized how to generalize Karatsuba&#39;s idea to become even more efficient, bringing the exponent in N^(log(3)/log(2)) down to log(2k - 1)/log(k), which gets as close to 1 as you like.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:17:58
<br>
Id: 1332810994540814336
<br>

<hr>

</p>
<p id="1332816759422902275"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe Yes, and keep in mind, it can&#39;t be that a population of 10 and a population of 10^5 have the same frequency, but rather, it needs to be that 10 is about ten thousand times as frequent as 10^5, in keeping with [10, 11) being ten thousand times smaller than [10^5, 1.1 * 10^5).</p></blockquote>
<br>
Timestamp: 2020-11-28 22:40:52
<br>
Id: 1332816759422902275
<br>

<hr>

</p>
<p id="1332817691636273152"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe Perhaps we are simply not inclined to say of distributions that they &quot;span many orders of magnitude&quot; unless it feels like they assign about a uniform probability to each order of magnitude, making them log-uniform.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:44:34
<br>
Id: 1332817691636273152
<br>

<hr>

</p>
<p id="1332817753263251457"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@JoeMoviegoer @sigfpe So that even a uniform distribution from 1 to a trillion is not the sort of thing that one is naturally inclined to say &quot;spans many orders of magnitude&quot;, since 90% of the distribution has twelve digits and only 10% of it is smaller.</p></blockquote>
<br>
Timestamp: 2020-11-28 22:44:49
<br>
Id: 1332817753263251457
<br>

<hr>

</p>
<p id="1332823077256900612"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@db_Math I never know good references for anything, but honestly, I think you could pick up the key ideas from Wikipedia, though it takes some skill to know how to find the key idea in a Wikipedia math article instead of being distracted by everything else.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:5:58
<br>
Id: 1332823077256900612
<br>

<hr>

</p>
<p id="1332824151929839624"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@db_Math See https://t.co/hs9KmMON7S in general and see https://t.co/yQ6GXq0Y6K for Toom&#39;s method in particular. The Wikipedia writeups on Fast Fourier Transform approaches bog down the key ideas a bit more with very particular details, but perhaps you really want those details.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:10:15
<br>
Id: 1332824151929839624
<br>

<hr>

</p>
<p id="1332835660995104776"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">@mirrorsaw_ Biden won. We achieved our goal and must move on to something else.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:55:59
<br>
Id: 1332835660995104776
<br>

<hr>

</p>
<p id="1332836115401814018"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Which is fine, but I fear I am in so doing contributing to everyone&#39;s mistaken impression as to what math is about, the general impression that math is specifically about numbers and numbery things and maybe some geometry.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:57:47
<br>
Id: 1332836115401814018
<br>

<hr>

</p>
<p id="1332836114466541568"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Relatively little of the times when I discuss math on here involve me discussing my ostensible specialization (logic), they&#39;re usually on some arithmetic thing instead. They could be about a million things but usually end up being about that, because that&#39;s what someone prompts.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:57:47
<br>
Id: 1332836114466541568
<br>

<hr>

</p>
<p id="1332836117356359680"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">I&#39;m all out of post ideas, so this is the slop you pigs get.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:57:47
<br>
Id: 1332836117356359680
<br>

<hr>

</p>
<p id="1332836116387536898"
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Numbers come up a lot because numbers happen to be ubiquitously useful, but math is about whatever. Alas, my merely saying &quot;math is about whatever&quot; is not very useful for conveying what I mean by that. C&#39;est la vie, some other time I&#39;ll say something else better.</p></blockquote>
<br>
Timestamp: 2020-11-28 23:57:47
<br>
Id: 1332836116387536898
<br>

<hr>

</p>
</body>
</html>
